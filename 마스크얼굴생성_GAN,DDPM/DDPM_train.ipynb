{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPM\n",
    "\n",
    "코드 참고 : https://github.com/awjuliani/pytorch-diffusion\n",
    "\n",
    "https://github.com/tcapelle/Diffusion-Models-pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "할거\n",
    "스케줄링\n",
    "val -> FID 계산하기 (fid.py에서 import해서 쓰면 될듯?)\n",
    "메모리 오류나면... batch 줄이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "c:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from model import DiffusionModel\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_steps = 1000\n",
    "max_epoch = 10\n",
    "img_size = 64\n",
    "img_channel = 1\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 18000\n",
      "Class names: ['with_mask', 'without_mask']\n"
     ]
    }
   ],
   "source": [
    "# Dataloader만들기\n",
    "\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5)) # normalization\n",
    "])\n",
    "\n",
    "data_dir = './Face-Mask-Classification-20000-Dataset/'\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "print('Train dataset size:', len(train_dataset))\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "print('Class names:', class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DiffusionModel(img_size*img_size, diffusion_steps, img_channel, n_class=2, light=False)\n",
    "checkpoint = torch.load('DDPM_BASE.ckpt')       # lr : 2e-5 에서 epoch당 0.8씩 11에포크 돌린거\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# \"global_step\" 기준으로 마지막 10개의 checkpoint만 저장\n",
    "# make sure you log it inside your LightningModule\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"results/DDPM/\",\n",
    "    #save_top_k=1,\n",
    "    save_on_train_epoch_end = True,\n",
    "    #filename=\"sample-mnist-{epoch:02d}-{global_step}\",\n",
    "    filename=\"sample-mnist-{epoch:02d}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type          | Params\n",
      "---------------------------------------------\n",
      "0  | label_emb | Embedding     | 512   \n",
      "1  | inc       | DoubleConv    | 37.7 K\n",
      "2  | down1     | Down          | 328 K \n",
      "3  | down2     | Down          | 1.2 M \n",
      "4  | down3     | Down          | 2.4 M \n",
      "5  | up1       | Up            | 6.2 M \n",
      "6  | up2       | Up            | 1.6 M \n",
      "7  | up3       | Up            | 422 K \n",
      "8  | sa1       | SelfAttention | 99.6 K\n",
      "9  | sa2       | SelfAttention | 395 K \n",
      "10 | sa3       | SelfAttention | 395 K \n",
      "11 | sa4       | SelfAttention | 99.6 K\n",
      "12 | sa5       | SelfAttention | 25.2 K\n",
      "13 | sa6       | SelfAttention | 25.2 K\n",
      "14 | bot1      | DoubleConv    | 3.5 M \n",
      "15 | bot2      | DoubleConv    | 4.7 M \n",
      "16 | bot3      | DoubleConv    | 1.8 M \n",
      "17 | outc      | Conv2d        | 65    \n",
      "---------------------------------------------\n",
      "23.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.3 M    Total params\n",
      "93.328    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 4500/4500 [08:35<00:00,  8.74it/s, v_num=3]0epoch lr : 1.8e-06\n",
      "train/loss : 0.02556\n",
      "with_mask ------  val - w/ mask : 222.77677940278568\n",
      "without_mask ------  val - w/o mask : 245.12473816055098\n",
      "\n",
      "Epoch 1: 100%|██████████| 4500/4500 [08:36<00:00,  8.70it/s, v_num=3]1epoch lr : 1.62e-06\n",
      "train/loss : 0.02508\n",
      "with_mask ------  val - w/ mask : 225.78924016361566\n",
      "without_mask ------  val - w/o mask : 244.59731892000406\n",
      "\n",
      "Epoch 2: 100%|██████████| 4500/4500 [08:37<00:00,  8.70it/s, v_num=3]2epoch lr : 1.458e-06\n",
      "train/loss : 0.02516\n",
      "with_mask ------  val - w/ mask : 242.50139191662328\n",
      "without_mask ------  val - w/o mask : 233.97161604281416\n",
      "\n",
      "Epoch 3:   0%|          | 0/4500 [00:00<?, ?it/s, v_num=3]           "
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of the allocated memory 5.84 GiB is allocated by PyTorch, and 490.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39mmax_epoch,callbacks\u001b[39m=\u001b[39m[checkpoint_callback],default_root_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mresults/DDPM/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, train_dataloader)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:531\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    529\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[0;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 531\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    532\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    533\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     44\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     45\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:570\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[0;32m    561\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[0;32m    562\u001b[0m )\n\u001b[0;32m    564\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    565\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    566\u001b[0m     ckpt_path,\n\u001b[0;32m    567\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    568\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    569\u001b[0m )\n\u001b[1;32m--> 570\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    572\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    573\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:975\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    972\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    980\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1018\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1017\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1018\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1019\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1020\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:201\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 201\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[0;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:354\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(combined_loader)\n\u001b[0;32m    353\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 354\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[0;32m    132\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[0;32m    134\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    135\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:218\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    217\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], kwargs)\n\u001b[0;32m    219\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:185\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m         closure()\n\u001b[0;32m    180\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 185\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m), closure)\n\u001b[0;32m    187\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[0;32m    188\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:260\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[0;32m    259\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[0;32m    261\u001b[0m     trainer,\n\u001b[0;32m    262\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    263\u001b[0m     trainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[0;32m    264\u001b[0m     batch_idx,\n\u001b[0;32m    265\u001b[0m     optimizer,\n\u001b[0;32m    266\u001b[0m     train_step_and_backward_closure,\n\u001b[0;32m    267\u001b[0m )\n\u001b[0;32m    269\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:140\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 140\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    142\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    143\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\core\\module.py:1256\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[0;32m   1219\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1220\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1223\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1224\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1225\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer`\u001b[39;00m\n\u001b[0;32m   1226\u001b[0m \u001b[39m    calls the optimizer.\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[39m                    pg[\"lr\"] = lr_scale * self.learning_rate\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1256\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:155\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    154\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy\u001b[39m.\u001b[39moptimizer_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer, closure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[0;32m    159\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:225\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[1;32m--> 225\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39moptimizer_step(optimizer, model\u001b[39m=\u001b[39mmodel, closure\u001b[39m=\u001b[39mclosure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:114\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39mstep(closure\u001b[39m=\u001b[39mclosure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\optim\\optimizer.py:293\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    290\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 293\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    294\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    296\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\optim\\optimizer.py:36\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 36\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     37\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\optim\\adam.py:125\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 125\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    127\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    128\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:101\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[0;32m     90\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     91\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     92\u001b[0m     optimizer: Optimizer,\n\u001b[0;32m     93\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m     94\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m     95\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \n\u001b[0;32m     98\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    102\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclosure(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:135\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_zero_grad_fn()\n\u001b[0;32m    134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backward_fn(step_output\u001b[39m.\u001b[39;49mclosure_loss)\n\u001b[0;32m    137\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:232\u001b[0m, in \u001b[0;36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[1;34m(loss)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward_fn\u001b[39m(loss: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 232\u001b[0m     call\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer, \u001b[39m\"\u001b[39;49m\u001b[39mbackward\u001b[39;49m\u001b[39m\"\u001b[39;49m, loss, optimizer)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:287\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 287\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    290\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:200\u001b[0m, in \u001b[0;36mStrategy.backward\u001b[1;34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    198\u001b[0m closure_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpre_backward(closure_loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module)\n\u001b[1;32m--> 200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mbackward(closure_loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, optimizer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    202\u001b[0m closure_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpost_backward(closure_loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module)\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_backward(closure_loss)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:67\u001b[0m, in \u001b[0;36mPrecisionPlugin.backward\u001b[1;34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     51\u001b[0m     tensor: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m     56\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[0;32m     59\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m        \\**kwargs: Keyword arguments for the same purpose as ``*args``.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     model\u001b[39m.\u001b[39mbackward(tensor, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\pytorch_lightning\\core\\module.py:1046\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[1;34m(self, loss, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fabric\u001b[39m.\u001b[39mbackward(loss, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1045\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1046\u001b[0m     loss\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\_tensor.py:491\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    482\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    483\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    484\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    489\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    490\u001b[0m     )\n\u001b[1;32m--> 491\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    492\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    493\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\autograd\\__init__.py:204\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    201\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    205\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    206\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of the allocated memory 5.84 GiB is allocated by PyTorch, and 490.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=max_epoch,callbacks=[checkpoint_callback],default_root_dir='results/DDPM/')\n",
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "trainer.save_checkpoint('DDPM_FINAL.ckpt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = DiffusionModel(img_size*img_size, diffusion_steps, img_channel, n_class=2, light=False)\n",
    "checkpoint = torch.load('DDPM_lr.ckpt')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "model.cuda()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [15:30<00:00, 37.22s/it]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image as Display\n",
    "import tqdm\n",
    "\n",
    "# 마스크를 착용한 총 10 * 100개의 얼굴 이미지를 생성\n",
    "for i in tqdm.tqdm(range(25)):\n",
    "    # 랜덤 노이즈(noise) 및 랜덤 레이블(label) 샘플링\n",
    "    x = torch.randn((4, 1, 64, 64)).cuda()\n",
    "    generated_labels = torch.cuda.IntTensor(4).fill_(0)\n",
    "\n",
    "    # 이미지 생성\n",
    "    sample_steps = torch.arange(model.t_range-1, 0, -1).cuda()\n",
    "    for t in sample_steps:\n",
    "        x = model.denoise_sample(x, t,generated_labels)\n",
    "    for j in range(4):\n",
    "        save_image(x[j], f'./results/DDPM/with_mask/{i * 4 + j}.png', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [15:52<00:00, 38.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# 마스크를 착용안한 총 10 * 100개의 얼굴 이미지를 생성\n",
    "for i in tqdm.tqdm(range(25)):\n",
    "    # 랜덤 노이즈(noise) 및 랜덤 레이블(label) 샘플링\n",
    "    x = torch.randn((4, 1, 64, 64)).cuda()\n",
    "    generated_labels = torch.cuda.IntTensor(4).fill_(1)\n",
    "\n",
    "    # 이미지 생성\n",
    "    sample_steps = torch.arange(model.t_range-1, 0, -1).cuda()\n",
    "    for t in sample_steps:\n",
    "        x = model.denoise_sample(x, t,generated_labels)\n",
    "    for j in range(4):\n",
    "        save_image(x[j], f'./results/DDPM/without_mask/{i * 4 + j}.png', normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FID 평가\n",
    "\n",
    "11 에포크\n",
    "\n",
    "    with_mask : 145.9\n",
    "\n",
    "    without_mask : 158.2\n",
    "\n",
    "\n",
    "21 에포크\n",
    "\n",
    "    with_mask : 148.3\n",
    "    without_mask : 164.2\n",
    "\n",
    "노이즈 이미지 제거 후\n",
    "\n",
    "    with_mask : 127.4\n",
    "    without_mask : 158.8\n",
    "\n",
    "lr 감소 후 \n",
    "\n",
    "    with_mask : 81.8\n",
    "    without_mask : 171.9\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Fréchet Inception Distance (FID) 공식 \n",
    " \n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "즉, mu와 sigma가 비슷할 수록 낮게 나옴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for images in ./results/DDPM/with_mask\\*.png\n",
      "Looking for images in ./results/DDPM/with_mask\\*.jpg\n",
      "Looking for images in ./Face-Mask-Classification-20000-Dataset/test/with_mask\\*.png\n",
      "Looking for images in ./Face-Mask-Classification-20000-Dataset/test/with_mask\\*.jpg\n",
      "path1 - mu:(2048,) sig:(2048, 2048)\n",
      "path2 - mu:(2048,) sig:(2048, 2048)\n",
      "81.82679762908074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "c:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "!python ./pytorch-frechet-inception-distance/fid.py --path1 ./results/DDPM/with_mask --path2 ./Face-Mask-Classification-20000-Dataset/test/with_mask --batch-size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for images in ./results/DDPM/without_mask\\*.png\n",
      "Looking for images in ./results/DDPM/without_mask\\*.jpg\n",
      "Looking for images in ./Face-Mask-Classification-20000-Dataset/test/without_mask\\*.png\n",
      "Looking for images in ./Face-Mask-Classification-20000-Dataset/test/without_mask\\*.jpg\n",
      "path1 - mu:(2048,) sig:(2048, 2048)\n",
      "path2 - mu:(2048,) sig:(2048, 2048)\n",
      "171.93600636470836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "c:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "!python ./pytorch-frechet-inception-distance/fid.py --path1 ./results/DDPM/without_mask --path2 ./Face-Mask-Classification-20000-Dataset/test/without_mask --batch-size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for images in ./test1\\*.png\n",
      "Looking for images in ./test1\\*.jpg\n",
      "Looking for images in ./test2\\*.png\n",
      "Looking for images in ./test2\\*.jpg\n",
      "path1 - mu:(2048,) sig:(2048, 2048)\n",
      "path2 - mu:(2048,) sig:(2048, 2048)\n",
      "35.37429881681976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "c:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "!python ./pytorch-frechet-inception-distance/fid.py --path1 ./test1 --path2 ./test2 --batch-size 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without_mask train과 test의 FID는 42 나옴(랜덤으로 골랐을 때)\n",
    "\n",
    "한쪽을 resize(64,64)하면 164나옴\n",
    "\n",
    "둘다 resize하면 35나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with_mask_path = 'results/DDPM/with_mask'\n",
    "without_mask_path = 'results/DDPM/without_mask'\n",
    "\n",
    "for path in [with_mask_path,without_mask_path]:\n",
    "    file_list = os.listdir(path)\n",
    "    for i,file_name in enumerate(file_list):\n",
    "        os.rename(os.path.join(path,file_name), os.path.join(path,str(i+12333)+'.png'))\n",
    "    file_list = os.listdir(path)\n",
    "    for i,file_name in enumerate(file_list):\n",
    "        os.rename(os.path.join(path,file_name), os.path.join(path,str(i)+'.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "path = 'test2'\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(path,file_name)\n",
    "    image = cv2.imread(file_path)\n",
    "    \n",
    "    # 이미지를 목표 크기로 reshape\n",
    "    resized_image = cv2.resize(image, (64,64))\n",
    "    \n",
    "    cv2.imwrite(file_path, resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
